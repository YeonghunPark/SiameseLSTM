{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import time\n",
    "%matplotlib inline  \n",
    "\n",
    "def get_params(name, shape):\n",
    "    w = tf.get_variable(name=name + \"_w\", shape=shape,\n",
    "                       #initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                       initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                       dtype=tf.float32)\n",
    "    n_filter = shape[-1]\n",
    "    b = tf.get_variable(name=name + \"_b\", shape=n_filter, initializer = tf.constant_initializer(0), dtype=tf.float32)\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "def FC(x, w, b):\n",
    "    fc = tf.matmul(x, w)\n",
    "    return tf.nn.bias_add(fc, b)\n",
    "\n",
    "def siamese_model(data, keep_prob, phase_train=True):\n",
    "    with tf.variable_scope('siamese'):\n",
    "        \n",
    "        with tf.variable_scope('rnn_block_1'):\n",
    "            \n",
    "            B, R, T = data.get_shape()\n",
    "            B, R, T = int(B), int(R), int(T)\n",
    "            data_re = tf.reshape(data, [B*R, T])\n",
    "            data_re = tf.reshape(data_re, [B*R, T, 1])\n",
    "            layers = [tf.nn.rnn_cell.BasicLSTMCell(size) for size in [64, 64, 64, 64, 64, 64]]\n",
    "            layers_drop = [tf.nn.rnn_cell.DropoutWrapper(layer, input_keep_prob = keep_prob) for layer in layers]\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(layers_drop, state_is_tuple=True)\n",
    "            \n",
    "            batch = data_re.get_shape().as_list()[0]\n",
    "            time_length = data_re.get_shape().as_list()[1]\n",
    "            initial_state = cell.zero_state(batch, tf.float32)\n",
    "            \n",
    "            outputs, final_state = tf.nn.dynamic_rnn(cell, data_re, initial_state=initial_state, dtype=tf.float32)\n",
    "            last_outputs = outputs[:,99,:]\n",
    "            last_outputs = tf.squeeze(last_outputs)\n",
    "            last_output = tf.reshape(last_outputs, [B, R, 64])\n",
    "            \n",
    "            glob_avg_pool = tf.reduce_mean(last_output, 2)\n",
    "            w1, b1 = get_params(name='1_fc', shape=(glob_avg_pool.get_shape()[1], 100))\n",
    "            FC1 = FC(x=glob_avg_pool, w=w1, b=b1)\n",
    "            FC1_tanh = tf.nn.tanh(FC1, name=\"FC1_tanh\")\n",
    "            \n",
    "        return outputs, final_state, last_output, glob_avg_pool, w1, FC1, FC1_tanh\n",
    "\n",
    "def contrastive_loss(y, d, batch_size):\n",
    "    tmp= y *tf.square(d)\n",
    "    tmp2 = (1-y) *tf.square(tf.maximum((1 - d),0))\n",
    "    return tf.reduce_sum(tmp +tmp2)/batch_size/2\n",
    "        \n",
    "#REST1_data = np.load(\"./norm_REST1_S900_FIND_BOLD_raw.npy\")\n",
    "#REST2_data = np.load(\"./norm_REST2_S900_FIND_BOLD_raw.npy\")\n",
    "\n",
    "graph =tf.Graph()\n",
    "with graph.as_default():\n",
    "    global_step = tf.get_variable('global_step', [], initializer = tf.constant_initializer(0), trainable=False)\n",
    "    \n",
    "    batch_size = 16\n",
    "    data_size = REST1_corr.shape[1]\n",
    "    num_ROI = 141\n",
    "    crop_size = 100\n",
    "    \n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(batch_size*2, num_ROI, crop_size), name=\"X\")\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(batch_size), name=\"Y\")\n",
    "    lr = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
    "    \n",
    "    phase_train = tf.placeholder(dtype=tf.bool, name=\"phase_train\")\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "    \n",
    "    LSTM_out, LSTM_final, Last_o, GAP, weight, FC, FC_tanh = siamese_model(X, keep_prob, phase_train)\n",
    "    \n",
    "    latent_a = FC_tanh[:batch_size]\n",
    "    latent_b = FC_tanh[batch_size:]\n",
    "    distance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(latent_a,latent_b)),1,keep_dims=True))\n",
    "    distance = tf.div(distance, tf.add(tf.sqrt(tf.reduce_sum(tf.square(latent_a),1,keep_dims=True)),tf.sqrt(tf.reduce_sum(tf.square(latent_b),1,keep_dims=True))))\n",
    "    distance = tf.reshape(distance, [-1], name=\"distance\")\n",
    "\n",
    "\n",
    "    CR_loss = contrastive_loss(Y, distance, batch_size)\n",
    "    \n",
    "    temp_sim = tf.subtract(tf.ones_like(distance),tf.rint(distance), name=\"temp_sim\") #auto threshold 0.5\n",
    "    correct_predictions = tf.equal(temp_sim, Y)\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "    \n",
    "    # Call parameters\n",
    "    tvars = tf.trainable_variables()\n",
    "    w_vars = [var for var in tvars if 'w' in var.name]\n",
    "    \n",
    "    regularizer = 0\n",
    "    w_vars_shape = np.shape(w_vars)\n",
    "    \n",
    "    for i in range(w_vars_shape[0]):\n",
    "        regularizer_one = tf.nn.l2_loss(w_vars[i])\n",
    "        regularizer += regularizer_one\n",
    "    \n",
    "    loss = CR_loss + 0.1*regularizer\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    Grads = optimizer.compute_gradients(loss)\n",
    "    train = optimizer.apply_gradients(Grads)\n",
    "    \n",
    "    # Call model saver\n",
    "    saver = tf.train.Saver(keep_checkpoint_every_n_hours=8, max_to_keep=100)\n",
    "    ## Open Tensorflow session\n",
    "    sess = tf.InteractiveSession(graph=graph, config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pairs(X1, X2):\n",
    "    N ,R, T= X1.shape\n",
    "    M = X2.shape[0]\n",
    "    n_pair = 2*N\n",
    "    Pair_X = np.empty((n_pair, R, 100, 2))\n",
    "    Pair_Y = np.ones(n_pair)\n",
    "\n",
    "    for i in range(N):\n",
    "        Pair_X[i,:,:,0]= X1[i,:,0:100]\n",
    "        Pair_X[i,:,:,1]= X2[i,:,0:100]\n",
    "\n",
    "    for i in range(N,2*N):\n",
    "        Pair_X[i,:,:,0]= X1[i-N,:,0:100]\n",
    "        if i-N == 0:\n",
    "            Pair_X[i,:,:,1]= X2[i-1,:,0:100]\n",
    "        else:\n",
    "            Pair_X[i,:,:,1]= X2[i-N-1,:,0:100]\n",
    "        Pair_Y[i] = 0\n",
    "            \n",
    "    return Pair_X, Pair_Y\n",
    "\n",
    "def prepare_test(X1, X2, order):\n",
    "    N ,R, T= X1.shape\n",
    "    M = X2.shape[0]\n",
    "    n_pair = 100\n",
    "    Pair_X = np.empty((n_pair, R, 100, 2))\n",
    "    Pair_Y = np.ones(n_pair)\n",
    "\n",
    "    Pair_X[0,:,:,0]= X1[order,:,0:100]\n",
    "    Pair_X[0,:,:,1]= X2[order,:,0:100]\n",
    "\n",
    "    for i in range(1,100): \n",
    "        Pair_X[i,:,:,0]= X1[order,:,0:100]\n",
    "        \n",
    "        if order+i < 100:\n",
    "            Pair_X[i,:,:,1]= X2[order+i,:,0:100]\n",
    "        else:\n",
    "            Pair_X[i,:,:,1]= X2[order+i-100,:,0:100]\n",
    "        Pair_Y[i] = 0\n",
    "            \n",
    "    return Pair_X, Pair_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./HCP_ID_1_fold_test_idx.txt','rb') as f:\n",
    "    test_index = pickle.load(f)\n",
    "with open('./HCP_ID_1_fold_train_idx.txt','rb') as f:\n",
    "    train_index = pickle.load(f)\n",
    "with open('./HCP_ID_1_fold_valid_idx.txt','rb') as f:\n",
    "    valid_index = pickle.load(f)\n",
    "    \n",
    "Train_REST1_data = REST1_data[train_index,:,:]\n",
    "Train_REST2_data = REST2_data[train_index,:,:]\n",
    "\n",
    "Valid_REST1_data = REST1_data[valid_index,:,:]\n",
    "Valid_REST2_data = REST2_data[valid_index,:,:]\n",
    "\n",
    "Test_REST1_data = REST1_data[test_index,:,:]\n",
    "Test_REST2_data = REST2_data[test_index,:,:]\n",
    "\n",
    "train_data, train_label = prepare_pairs(Train_REST1_data, Train_REST2_data)\n",
    "num_train_trials = train_data.shape[0]\n",
    "print(num_train_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network_output():\n",
    "        \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    ax1.plot(Accuracy_list, 'b.-', alpha = .3)\n",
    "    ax1.set_ylabel('Training accuracy',  color='b')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylim((0.1,1.0))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(Accuracy_val_list, 'r.-', alpha = .3)\n",
    "    ax2.set_ylabel('Validation accuracy', color='r')\n",
    "    ax2.set_ylim((0.1,1.0))\n",
    "    plt.title('Training & Validation accuracy')\n",
    "    plt.show()\n",
    "    fig.savefig('./1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_acc_last.png',dpi=100)\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    ax1.plot(Loss_list, 'b.-', alpha = .3)\n",
    "    ax1.set_ylabel('Training loss',  color='b')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylim((0,1))\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(Loss_val_list, 'r.-', alpha = .3)\n",
    "    ax2.set_ylabel('Validation loss', color='r')\n",
    "    ax2.set_ylim((0,1))\n",
    "    plt.title('Training & Validation loss')\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig('./1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_loss_last.png',dpi=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_list = []\n",
    "CR_Loss_list = []\n",
    "Accuracy_list = []\n",
    "\n",
    "Loss_val_list = []\n",
    "Accuracy_val_list = []\n",
    "\n",
    "Loss_test_list = []\n",
    "Accuracy_test_list = []\n",
    "\n",
    "pre_val_acc = 0.1\n",
    "pre_val_loss = 1\n",
    "total_epoch = 1000\n",
    "print(\"Begin Training\")\n",
    "total_batch = int((num_train_trials) / batch_size)\n",
    "epoch = 0\n",
    "learning_rate = 1e-5\n",
    "\n",
    "while epoch < total_epoch:\n",
    "    \n",
    "    Acc_avg = []\n",
    "    Loss_avg = []\n",
    "    CR_Loss_avg = []\n",
    "    Acc_val_avg = []\n",
    "    Loss_val_avg = []\n",
    "    CR_Loss_val_avg = []\n",
    "    Acc_test_avg = []\n",
    "    Loss_test_avg = []\n",
    "    CR_Loss_test_avg = []\n",
    "    \n",
    "    rand_idx = np.random.permutation(num_train_trials)\n",
    "    for batch in tqdm.tqdm(range(total_batch)):\n",
    "        batch_x = np.empty(shape=(batch_size*2, num_ROI, crop_size))\n",
    "        batch_y = np.empty(shape=(batch_size))\n",
    "        \n",
    "        position = rand_idx[batch*batch_size : (batch+1)*batch_size]\n",
    "                \n",
    "        batch_x[:batch_size] = train_data[position, :, :, 0]\n",
    "        batch_x[batch_size:] = train_data[position, :, :, 1]\n",
    "        batch_y = train_label[position]\n",
    "\n",
    "        _, loss_, CR_loss_ = sess.run([train, loss, CR_loss], {X: batch_x, Y: batch_y, lr: learning_rate, keep_prob: 0.8, phase_train: True})\n",
    "        Acc_value= sess.run(accuracy, {X:batch_x, Y: batch_y, keep_prob:1.0, phase_train: False})\n",
    "        \n",
    "        Acc_avg.append(Acc_value)\n",
    "        Loss_avg.append(loss_)\n",
    "        CR_Loss_avg.append(CR_loss_)\n",
    "    \n",
    "    Accuracy_list.append(np.mean(Acc_avg))\n",
    "    Loss_list.append(np.mean(Loss_avg))\n",
    "    CR_Loss_list.append(np.mean(CR_Loss_avg))\n",
    "    IPython.display.clear_output()\n",
    "    print(\"%dth Epoch Training Accuracy: %f\" %(epoch + 1, np.mean(Acc_avg)))\n",
    "    print(\"%dth Epoch Training Loss: %f\" %(epoch + 1, np.mean(Loss_avg)))\n",
    "    print(\"%dth Epoch Training Sigmoid Loss: %f\" %(epoch + 1, np.mean(CR_Loss_avg)))\n",
    "    \n",
    "    print(\"Model Saving\")\n",
    "    saver.save(sess, \"./1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_last.tfmod\")\n",
    "    f = open(''.join(['1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_last',str(epoch).zfill(5)]), 'w')\n",
    "    f.close()\n",
    "    \n",
    "    Total_loss_valid = []\n",
    "    count = 0\n",
    "    for i in range(100):\n",
    "        Acc_valid_data, Acc_valid_label = prepare_test(Valid_REST1_data, Valid_REST2_data, i)\n",
    "        valid_idx =np.arange(100)\n",
    "        \n",
    "        Total_dist_valid = []\n",
    "        for batch in range(int(100/batch_size)):\n",
    "            #print(batch)\n",
    "            batch_x_valid = np.empty(shape=(batch_size*2, num_ROI, crop_size))\n",
    "            batch_y_valid = np.empty(shape=(batch_size))\n",
    "        \n",
    "            position = valid_idx[batch*batch_size : (batch+1)*batch_size]\n",
    "        \n",
    "            batch_x_valid[:batch_size] = Acc_valid_data[position, :, :, 0]\n",
    "            batch_x_valid[batch_size:] = Acc_valid_data[position, :, :, 1]\n",
    "            batch_y_valid = Acc_valid_label[position]\n",
    "            temp_dist_valid, loss_valid = sess.run([distance,loss], {X:batch_x_valid, Y:batch_y_valid, keep_prob:1.0, phase_train: False})\n",
    "                                                               \n",
    "            Total_dist_valid.extend(temp_dist_valid)\n",
    "            Total_loss_valid.append(loss_valid)\n",
    "        batch_x_valid = np.empty(shape=(batch_size*2, num_ROI, crop_size))\n",
    "        batch_y_valid = np.empty(shape=(batch_size))\n",
    "        batch_x_valid[:batch_size] = Acc_valid_data[100-batch_size:100, :, :, 0]\n",
    "        batch_x_valid[batch_size:] = Acc_valid_data[100-batch_size:100, :, :, 1]\n",
    "        batch_y_valid = Acc_valid_label[100-batch_size:100]\n",
    "        temp_dist_valid, loss_valid = sess.run([distance,loss], {X:batch_x_valid, Y:batch_y_valid, keep_prob:1.0, phase_train: False})\n",
    "    \n",
    "        Total_dist_valid.extend(temp_dist_valid[12:]) \n",
    "        Total_loss_valid.append(loss_valid)\n",
    "        \n",
    "        ans = np.argmin(Total_dist_valid)\n",
    "        if ans == 0:\n",
    "            count += 1\n",
    "    Acc_valid = count/100.0\n",
    "    Accuracy_val_list.append(Acc_valid)\n",
    "    Loss_val_list.append(np.mean(Total_loss_valid))\n",
    "    print(\"Total Valid Accuracy: %4f\" % (Acc_valid))\n",
    "    print(\"Total Valid Max Accuracy: %4f\" % (max(Accuracy_val_list)))\n",
    "    print(\"TotalValid Loss: %4f\" % (np.mean(Total_loss_valid)))\n",
    "    \n",
    "    validation_acc = Acc_valid\n",
    "    if validation_acc >= pre_val_acc:\n",
    "        pre_val_acc = validation_acc\n",
    "        print(\"save max weight at %s acc\"%(validation_acc))\n",
    "        saver.save(sess, ''.join(['max_weight/1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_model.tfmod']))\n",
    "        f = open(''.join(['max_weight/1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_last_model_',str(epoch).zfill(5)]), 'w')\n",
    "        f.close()\n",
    " \n",
    "    validation_loss = np.mean(Total_loss_valid)\n",
    "    if validation_loss <= pre_val_loss:\n",
    "        pre_val_loss = validation_loss\n",
    "        print(\"save min weight at %s loss\"%(validation_loss))\n",
    "        saver.save(sess, ''.join(['min_weight/1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_model.tfmod']))\n",
    "        f = open(''.join(['min_weight/1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_last_model_',str(epoch).zfill(5)]), 'w')\n",
    "        f.close()\n",
    "    #print(\"End Training\\n\")\n",
    "    plot_network_output()\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify test_data with weight of maximum accuracy of validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver = tf.train.import_meta_graph('max_weight/1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_model.tfmod.meta')\n",
    "#tf.train.Saver.restore(saver, sess, 'max_weight/1fold_LSTM_64x6_GAP_FC_no_drop_0.8_lr1e-5_model.tfmod')\n",
    "saver = tf.train.import_meta_graph('./HCP_ID_1fold_trained_weight.tfmod.meta')\n",
    "tf.train.Saver.restore(saver, sess, './HCP_ID_1fold_trained_weight.tfmod')\n",
    "\n",
    "Test_REST1_data = np.load(\"./norm_REST1_S900_test.npy\")\n",
    "Test_REST1_data = np.load(\"./norm_REST2_S900_test.npy\")\n",
    "\n",
    "Total_loss_test =[]\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    Acc_test_data, Acc_test_label = prepare_test(Test_REST1_data, Test_REST2_data, i)\n",
    "    test_idx =np.arange(100)\n",
    "    \n",
    "    Total_dist_test = []\n",
    "    for batch in range(int(100/batch_size)):\n",
    "        batch_x_test = np.empty(shape=(batch_size*2, num_ROI, crop_size))\n",
    "        batch_y_test = np.empty(shape=(batch_size))\n",
    "        \n",
    "        position = test_idx[batch*batch_size : (batch+1)*batch_size]\n",
    "        \n",
    "        batch_x_test[:batch_size] = Acc_test_data[position, :, :, 0]\n",
    "        batch_x_test[batch_size:] = Acc_test_data[position, :, :, 1]\n",
    "        batch_y_test = Acc_test_label[position]\n",
    "        temp_dist_test, loss_test = sess.run([distance, loss], {X:batch_x_test, Y:batch_y_test, keep_prob:1.0, phase_train: False})\n",
    "                                                               \n",
    "        Total_dist_test.extend(temp_dist_test)\n",
    "        Total_loss_test.append(loss_test)\n",
    "                \n",
    "    batch_x_test = np.empty(shape=(batch_size*2, num_ROI, crop_size))\n",
    "    batch_y_test = np.empty(shape=(batch_size))\n",
    "    batch_x_test[:batch_size] = Acc_test_data[100-batch_size:100, :, :, 0]\n",
    "    batch_x_test[batch_size:] = Acc_test_data[100-batch_size:100, :, :, 1]\n",
    "    batch_y_test = Acc_test_label[100-batch_size:100]\n",
    "    temp_dist_test, loss_test = sess.run([distance, loss], {X:batch_x_test, Y:batch_y_test, keep_prob:1.0, phase_train: False})\n",
    "    \n",
    "    Total_dist_test.extend(temp_dist_test[12:]) \n",
    "    Total_loss_test.append(loss_test)\n",
    "            \n",
    "    ans = np.argmin(Total_dist_test)\n",
    "\n",
    "    if ans == 0:\n",
    "        count += 1\n",
    "                \n",
    "print(\"Total Test Accuracy: %4f\" % (count/100.0))\n",
    "print(\"Total Test Loss: %4f\" % (np.mean(Total_loss_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
